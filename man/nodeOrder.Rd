% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/downstream-analysis.R
\name{nodeOrder}
\alias{nodeOrder}
\title{Order nodes and modules within a network.}
\usage{
nodeOrder(data = NULL, correlation, network, moduleAssignments = NULL,
  modules = NULL, backgroundLabel = "0", discovery = NULL, test = NULL,
  nCores = NULL, na.rm = FALSE, orderModules = TRUE, simplify = TRUE,
  verbose = TRUE)
}
\arguments{
\item{data}{a list of numeric matrices. Each entry of the list corresponds to
a dataset and contains the data used to infer the interaction network
between variables (e.g. genes). Expects matrix columns to correspond to
variables and matrix rows to correspond to samples.}

\item{correlation}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the correlation between 
each pair of variables in the dataset.}

\item{network}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the network edge weights 
between each pair of variables in the dataset.}

\item{moduleAssignments}{a vector containing the module each variable belongs
to in the discovery dataset. If there are multiple discovery datasets 
then this argument should be a list of such vectors.}

\item{modules}{a list of vectors, one for each \code{discovery} dataset, 
of modules to perform the analysis on. The default is to analyse all modules
in each \code{discovery} dataset, with the exception of those specified in 
\code{backgroundLabel}.}

\item{backgroundLabel}{a single label that nodes that do not belong to any
module are assigned. The default is "0".}

\item{discovery}{a vector of names or indices denoting the discovery dataset(s).}

\item{test}{a list of vectors of names or indices denoting the test datasets
for each \code{discovery} dataset. Alternatively can be provided as vector
if the test datasets are the same for all 'discovery' datasets (e.g. for 
performing a pairwise comparison).}

\item{nCores}{number of cores to parallelise the calculation of network 
properties over. Ignored if the user has already registered a parallel 
backend.If \code{NULL} (default) the maximum number of cores on the machine
will be used.}

\item{na.rm}{logical; If \code{TRUE}, nodes and moduels present in the 
\code{discovery} dataset but missing from the test dataset are excluded. If
\code{FALSE}, missing nodes and modules are put last in the ordering.}

\item{orderModules}{logical; if \code{TRUE} modules ordered by clustering 
their summary vectors. If \code{FALSE} modules are returned in the order
provided.}

\item{simplify}{logical; if \code{TRUE}, simplify the structure of the output
list if possible (see Return Value).}

\item{verbose}{logical; should progress be reported? Default is \code{TRUE}.}
}
\value{
A nested list structure. At the top level, the list has one element per 
 \code{'discovery'} dataset. Each of these elements is a list that has one
 element per \code{'test'} dataset analysed for that \code{'discovery'} 
 dataset. Each of these elements is a list that has one element per 
 \code{'modules'} specified, containing a vector of node names for the
 requested module. If \code{simplify = TRUE}, then there will be a single
 vector of node names for each \code{'test'} dataset.
}
\description{
Order nodes in descending order of \emph{weighted degree} and order 
modules by the similarity of their summary vectors.
}
\details{
\subsection{Input data structure:}{
  The \link[=modulePreservation]{preservation of network modules} in a second
  dataset is quantified by measuring the preservation of topological
  properties between the \emph{discovery} and \emph{test} datasets. These 
  properties are calculated not only from the interaction networks inferred
  in each dataset, but also from the data used to infer those networks (e.g.
  gene expression data) as well as the correlation structure between 
  variables/nodes. Thus, all functions in the \code{NetRep} package have the 
  following arguments: 
  \itemize{
    \item{\code{network}:}{
      a list of interaction networks, one for each dataset.
    }
    \item{\code{data}:}{
      a list of data matrices used to infer those networks, one for each 
      dataset.
    }
    \item{\code{correlation}:}{
     a list of matrices containing the pairwise correlation coefficients 
     between variables/nodes in each dataset.
    } 
    \item{\code{moduleAssignments}:}{
     a list of vectors, one for each \emph{discovery} dataset, containing 
     the module assignments for each node in that dataset.
    }
    \item{\code{modules}:}{
     a list of vectors, one for each \emph{discovery} dataset, containing
     the names of the modules from that dataset to analyse.  
    }
    \item{\code{discovery}:}{
      a vector indicating the names or indices of the previous arguments' 
      lists to use as the \emph{discovery} dataset(s) for the analyses.
    }
    \item{\code{test}:}{
      a list of vectors, one vector for each \emph{discovery} dataset, 
      containing the names or indices of the \code{network}, \code{data}, and 
      \code{correlation} argument lists to use as the \emph{test} dataset(s) 
      for the analysis of each \emph{discovery} dataset.
    }
  }
  
  The formatting of these arguments is not strict: each function will attempt
  to make sense of the user input. For example, if there is only one 
  \code{discovery} dataset, then input to the \code{moduleAssigments} and 
  \code{test} arguments may be vectors, rather than lists. If the 
  \code{nodeOrder} are being calculate within the \emph{discovery} or
  \emph{test} datasets, then the \code{discovery} and \code{test} arguments do
  not need to be specified, and the input matrices for the \code{network},
  \code{data}, and \code{correlation} arguments do not need to be wrapped in
  a list.
}
\subsection{'bigMatrix' vs. 'matrix' input data:}{
  Although the function expects \code{\link{bigMatrix}} 
  data, regular 'matrix' objects are also accepted. In this case, the 
  'matrix' data is temporarily converted to 'bigMatrix' by the function. This
  conversion process involves writing out each matrix as a binary file on 
  disk, which can take a long time for large datasets. It is strongly 
  recommended for the user to store their data as 'bigMatrix' objects, as the
  \link{modulePreservation} function, \link[=plotModule]{plotting} 
  \link[=plotTopology]{functions}, \link[=nodeOrder]{node} and 
  \link[=sampleOrder]{sample} ordering functions also expect 'bigMatrix'
  objects. Further, 'bigMatrix' objects have a number of benefits, including 
  instantaneous load time from any future R session, and parallel access from
  mutliple independent R sessions. Methods are provided for 
  converting to, loading in, and writing out \code{\link{bigMatrix}} objects.
}
}
\examples{
\dontrun{
# load in example data, correlation, and network matrices for a discovery and test dataset:
data("NetRep")

# Convert them to the 'bigMatrix' format:
discovery_data <- as.bigMatrix(discovery_data)
discovery_correlation <- as.bigMatrix(discovery_correlation)
discovery_network <- as.bigMatrix(discovery_network)
test_data <- as.bigMatrix(test_data)
test_correlation <- as.bigMatrix(test_correlation)
test_network <- as.bigMatrix(test_network)

# Set up input lists for each input matrix type across datasets:
data_list <- list(discovery=discovery_data, test=test_data)
correlation_list <- list(discovery=discovery_correlation, test=test_correlation)
network_list <- list(discovery=discovery_network, test=test_network)
labels_list <- list(discovery=module_labels)

# Sort nodes by module similarity and node degree
nodes <- nodeOrder(
  data=data_list, correlation=correlation_list, network=network_list, 
  moduleAssignments=labels_list
)
}

}
\references{
\enumerate{
   \item{
     Langfelder, P., Mischel, P. S. & Horvath, S. \emph{When is hub gene 
     selection better than standard meta-analysis?} PLoS One \strong{8}, 
     e61505 (2013).
   }
}
}
\seealso{
\code{\link{networkProperties}}
}

