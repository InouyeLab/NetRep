% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/downstream-analysis.R
\name{networkProperties}
\alias{networkProperties}
\title{Calculate the topological properties for a network module}
\usage{
networkProperties(data = NULL, correlation, network,
  moduleAssignments = NULL, modules = NULL, backgroundLabel = "0",
  discovery = NULL, test = NULL, nCores = NULL, simplify = TRUE,
  verbose = TRUE)
}
\arguments{
\item{data}{a list of numeric matrices. Each entry of the list corresponds to
a dataset and contains the data used to infer the interaction network
between variables (e.g. genes). Expects matrix columns to correspond to
variables and matrix rows to correspond to samples.}

\item{correlation}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the correlation between 
each pair of variables in the dataset.}

\item{network}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the network edge weights 
between each pair of variables in the dataset.}

\item{moduleAssignments}{a vector containing the module each variable belongs
to in the discovery dataset. If there are multiple discovery datasets 
then this argument should be a list of such vectors.}

\item{modules}{a list of vectors, one for each \code{discovery} dataset, 
of modules to perform the analysis on. The default is to analyse all modules
in each \code{discovery} dataset, with the exception of those specified in 
\code{backgroundLabel}.}

\item{backgroundLabel}{a single label that nodes that do not belong to any
module are assigned. The default is "0".}

\item{discovery}{a vector of names or indices denoting the discovery dataset(s).}

\item{test}{a list of vectors of names or indices denoting the test datasets
for each \code{discovery} dataset. Alternatively can be provided as vector
if the test datasets are the same for all 'discovery' datasets (e.g. for 
performing a pairwise comparison).}

\item{nCores}{number of cores to parallelise the calculation of network 
properties over. Ignored if the user has already registered a parallel 
backend.If \code{NULL} (default) the maximum number of cores on the machine
will be used.}

\item{simplify}{logical; if \code{TRUE}, simplify the structure of the output
list if possible (see Return Value).}

\item{verbose}{logical; should progress be reported? Default is \code{TRUE}.}
}
\value{
A nested list structure. At the top level, the list has one element per 
 \code{'discovery'} dataset. Each of these elements is a list that has one
 element per \code{'test'} dataset analysed for that \code{'discovery'} 
 dataset. Each of these elements is a list that has one element per 
 \code{'modules'} specified. Each of these is a list containing the following  
 objects:
 \itemize{
   \item{\code{'degree'}:}{
     The weighted within-module degree: the sum of edge weights for each 
     node in the module.
   }
   \item{\code{'avgWeight'}:}{
     The average edge weight within the module.
   }
 }
 If the \code{'data'} used to infer the \code{'test'} network is provided  
 then the following are also returned:
 \itemize{
   \item{\code{'summary'}:}{
     A vector summarising the module across each sample. This is calculated 
     as the first eigenvector of the module from a principal component 
     analysis.
   }
   \item{\code{'contribution'}:}{
     The \emph{node contribution}: the similarity between each node and the
     \emph{module summary profile} (\code{'summary'}).
   }
   \item{\code{'coherence'}:}{
     The proportion of module variance explained by the \code{'summary'}
     vector.
   }
 }
 For example, \code{results[[1]][[2]][["blue"]][["degree"]]} is a vector
 containing the \emph{weighted node degree} for the "blue" module from the
 dataset 1, as calculated in dataset 2. module preservation p-values when
 assessing the preservation of modules from dataset 1 in dataset 2. If
 \code{simplify = TRUE} then the list structure will be simplified where
 possible.
}
\description{
Calculates the network properties used to assess module preservation for one
or more modules in a user specified dataset.
}
\details{
\subsection{Input data structure:}{
  This function allows for input data formatted in a number of ways. Where 
  there are multiple datasets of interest (e.g. multiple tissues, locations, 
  or a discovery dataset and an independent test dataset) the arguments 
  \code{data}, \code{correlation}, and \code{network} should be
  \code{\link[=list]{lists}} where each element contains the matrix data for 
  each respective dataset. Alternatively, if only one dataset is of interest, 
  the \code{data}, \code{correlation}, and \code{network} arguments
  will also each accept a 'matrix' object.
  
  Similarly, the \code{moduleAssignments} argument expects a list of named
  vectors, which denote the module each variable belongs to in the discovery
  dataset. If module discovery has only been performed in one dataset, then 
  the \code{moduleAssignments} argument will also accept a named vector.
  
  The \code{discovery} arguments specifies which dataset the \code{modules} 
  of interest were discovered in, and the \code{test} argument specifies 
  which dataset to calculate the network properties in. These arguments are
  ignored if data is provided for only one dataset.
}
\subsection{'bigMatrix' vs. 'matrix' input data:}{
  Although the function expects \code{\link{bigMatrix}} 
  data, regular 'matrix' objects are also accepted. In this case, the 
  'matrix' data is temporarily converted to 'bigMatrix' by the function. This
  conversion process involves writing out each matrix as a binary file on 
  disk, which can take a long time for large datasets. It is strongly 
  recommended for the user to store their data as 'bigMatrix' objects, as the
  \link{modulePreservation} function, \link[=plotModule]{plotting} 
  \link[=plotTopology]{functions}, \link[=nodeOrder]{node} and 
  \link[=sampleOrder]{sample} ordering functions also expect 'bigMatrix'
  objects. Further, 'bigMatrix' objects have a number of benefits, including 
  instantaneous load time from any future R session, and parallel access from
  mutliple independent R sessions. Methods are provided for 
  converting to, loading in, and writing out \code{\link{bigMatrix}} objects.
}
}
\examples{
\dontrun{
# load in example data, correlation, and network matrices for a discovery and test dataset:
data("NetRep")

# Convert them to the 'bigMatrix' format:
discovery_data <- as.bigMatrix(discovery_data)
discovery_correlation <- as.bigMatrix(discovery_correlation)
discovery_network <- as.bigMatrix(discovery_network)
test_data <- as.bigMatrix(test_data)
test_correlation <- as.bigMatrix(test_correlation)
test_network <- as.bigMatrix(test_network)

# Set up input lists for each input matrix type across datasets:
data_list <- list(discovery=discovery_data, test=test_data)
correlation_list <- list(discovery=discovery_correlation, test=test_correlation)
network_list <- list(discovery=discovery_network, test=test_network)
labels_list <- list(discovery=module_labels)

# Calculate the topological properties of all network modules in the discovery dataset
props <- networkProperties(
  data=data_list, correlation=correlation_list, network=network_list, 
  moduleAssignments=labels_list
)
  
# Calculate the topological properties in the test dataset for the same modules
test_props <- networkProperties(
  data=data_list, correlation=correlation_list, network=network_list, 
  moduleAssignments=labels_list, discovery="discovery", test="test"
)
}

}
\seealso{
\link[=nodeOrder]{Getting nodes ordered by degree.}, and
  \link[=sampleOrder]{Ordering samples by module summary}
}

