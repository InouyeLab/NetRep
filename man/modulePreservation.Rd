% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modulePreservation.R
\name{modulePreservation}
\alias{modulePreservation}
\title{Replication and preservation of network modules across datasets}
\usage{
modulePreservation(data = NULL, correlation, network, moduleAssignments,
  modules = NULL, backgroundLabel = "0", discovery = 1, test = 2,
  selfPreservation = FALSE, nCores = NULL, nPerm = NULL,
  null = "overlap", alternative = "greater", statCorMethod = "pearson",
  simplify = TRUE, verbose = TRUE)
}
\arguments{
\item{data}{a list of matrices, one for each dataset. Each 
entry of the list should be a numeric \code{\link{bigMatrix}} of data used
to infer the interaction network for that dataset. The columns should
correspond to variables in the data (nodes in the network) and rows to 
samples in that dataset.}

\item{correlation}{a list of matrices, one for each dataset. Each entry of
the list should be a \eqn{n * n} numeric \code{\link{bigMatrix}} where each
element contains the correlation coefficient between nodes \eqn{i} and
\eqn{j} in data used to infer the interaction network for that dataset.}

\item{network}{a list of interaction networks, one for each dataset. Each 
entry of the list should be a \eqn{n * n} numeric \code{\link{bigMatrix}}
where each element contains the edge weight between nodes \eqn{i} and 
\eqn{j} in the inferred network for that dataset.}

\item{moduleAssignments}{a list of vectors, one for each \emph{discovery} 
dataset, containing the module assignments for each node in that dataset.}

\item{modules}{a list of vectors, one for each \code{discovery} dataset, 
of modules to perform the analysis on. If unspecified, all modules
in each \code{discovery} dataset will be analysed, with the exception of 
those specified in \code{backgroundLabel} argument.}

\item{backgroundLabel}{a single label given to nodes that do not belong to 
any module in the \code{moduleAssignments} argument.}

\item{discovery}{a vector of names or indices denoting the \emph{discovery}
dataset(s) in the \code{data}, \code{correlation}, \code{network}, 
\code{moduleAssignments}, \code{modules}, and \code{test} lists.}

\item{test}{a list of vectors, one for each \code{discovery} dataset,
of names or indices denoting the \emph{test} dataset(s) in the \code{data}, 
\code{correlation}, and \code{network} lists.}

\item{selfPreservation}{logical; if \code{FALSE} (default) then module 
preservation analysis will not be performed within a dataset (\emph{i.e.} 
where the \code{discovery} and \code{test} datasets are the same).}

\item{nCores}{number of cores to parallelise the permutation procedure over.
Ignored if the user has already registered a parallel backend. If 
\code{NULL} (default) the maximum number of cores on the machine will be 
used.}

\item{nPerm}{number of permutations to use. If not specified, the number of 
permutations will be automatically determined (see details).}

\item{null}{variables to include when generating the null distributions. 
Must be either "overlap" or "all" (see details).}

\item{alternative}{The type of module preservation test to perform. Must be 
one of "greater" (default), "less" or "two.sided" (see details).}

\item{statCorMethod}{character vector indicating method to use when calculating 
the correlation based statistics (see details). Must be one of "pearson", 
"spearman", or "kendall". If the WGCNA package is installed then "bicor" 
may also be specified as an option (see \code{\link[WGCNA]{bicor}}).}

\item{simplify}{logical; if \code{TRUE}, simplify the structure of the output
list if possible (see Return Value).}

\item{verbose}{logical; should progress be reported? Default is \code{TRUE}.}
}
\value{
A nested list structure. At the top level, the list has one element per 
 \code{'discovery'} dataset. Each of these elements is a list that has one
 element per \code{'test'} dataset analysed for that \code{'discovery'} 
 dataset. Each of these elements is also a list, containing the following
 objects:
 \itemize{
   \item{\code{observed}:}{
     A matrix of the observed values for the module preservation statistics.
     Rows correspond to modules, and columns to the module preservation
     statistics.
   }
   \item{\code{nulls}:}{
     A three dimensional array containing the values of the module 
     preservation statistics evaluated on random permutation of module 
     assignment in the test network. Rows correspond to modules, columns to
     the module preservation statistics, and the third dimension to the 
     permutations.
   }
   \item{\code{p.values}:}{
     A matrix of p-values for the \code{observed} module preservation 
     statistics as evaluated through a permutation test using the 
     corresponding values in \code{nulls}.
   }
   \item{\code{nVarsPresent}:}{
     A vector containing the number of variables that are present in the test
     dataset for each module.
   }
   \item{\code{propVarsPresent}:}{
     A vector containing the proportion of variables present in the test dataset
     for each module. Modules where this is less than 1 should be 
     investigated further before making judgements about preservation to 
     ensure that the missing variables are not the most connected ones.
   }
   \item{\code{contingency}:}{ 
     If \code{moduleAssignments} are present for both the \emph{discovery}
     and \emph{test} datasets, then a contingency table showing the overlap
     between modules across datasets is returned. Rows correspond to modules
     in the \emph{discovery} dataset, columns to modules in the \emph{test}
     dataset.
   }
 }
 
 For example, \code{results[[1]][[2]][["p.values"]]} is the matrix of 
 module preservation p-values when assessing the preservation of modules from
 dataset 1 in dataset 2. If \code{simplify = TRUE} then the list structure 
 will be simplified where possible.
}
\description{
Quantify the preservation of network modules (sub-graphs) in an independent
dataset through permutation testing on module topology. Seven network
statistics (see details) are calculated for each module and then tested by
comparing to distributions generated from their calculation on random subsets
in the test dataset.
}
\details{
\subsection{Input data structure:}{
  The preservation of network modules in a second dataset is quantified by
  measuring the preservation of topological properties between the
  \emph{discovery} and \emph{test} datasets. These properties are calculated
  not only from the interaction networks inferred in each dataset, but also
  from the data used to infer those networks (e.g. gene expression data) as
  well as the correlation structure between variables/nodes. Thus, all
  functions in the \code{NetRep} package have the following arguments:
  \itemize{
    \item{\code{network}:}{
      a list of interaction networks, one for each dataset.
    }
    \item{\code{data}:}{
      a list of data matrices used to infer those networks, one for each 
      dataset.
    }
    \item{\code{correlation}:}{
     a list of matrices containing the pairwise correlation coefficients 
     between variables/nodes in each dataset.
    } 
    \item{\code{moduleAssignments}:}{
     a list of vectors, one for each \emph{discovery} dataset, containing 
     the module assignments for each node in that dataset.
    }
    \item{\code{modules}:}{
     a list of vectors, one for each \emph{discovery} dataset, containing
     the names of the modules from that dataset to analyse.  
    }
    \item{\code{discovery}:}{
      a vector indicating the names or indices of the previous arguments' 
      lists to use as the \emph{discovery} dataset(s) for the analyses.
    }
    \item{\code{test}:}{
      a list of vectors, one vector for each \emph{discovery} dataset, 
      containing the names or indices of the \code{network}, \code{data}, and 
      \code{correlation} argument lists to use as the \emph{test} dataset(s) 
      for the analysis of each \emph{discovery} dataset.
    }
  }
  
  The formatting of these arguments is not strict: each function will attempt
  to make sense of the user input. For example, if there is only one 
  \code{discovery} dataset, then input to the \code{moduleAssigments} and 
  \code{test} arguments may be vectors, rather than lists. 
}
\subsection{'bigMatrix' input data:}{
  Although the \code{data}, \code{correlation}, and \code{network} arguments 
  expect data to be provided in the \code{\link{bigMatrix}}, they can be 
  provided as regular \code{\link[base]{matrix}} objects, in which case they 
  will be temporarily converted to \code{bigMatrix} objects. \strong{This is 
  not recommended}, as each matrix will be copied into memory of each 
  parallel R session, resulting in much higher memory usage (see section on
  Memory usage ), and increased computation time for the conversion and
  copying processes. It is therefore strongly recommended that the user save
  their data separately as \code{\link{bigMatrix}} objects prior to running
  the permutation procedure or using any other package function. This is also
  useful for other analyses, as \code{bigMatrix} objects can be
  instantaneously loaded into any future R session.
  
  Alternatively, the \code{data}, \code{correlation}, and \code{network} 
  arguments will also accept file paths to tabular data or \code{bigMatrix} 
  backingfiles.
}
\subsection{Memory usage:}{
  Provided there are no additional objects in the R session the
  permutation procedure will use only the memory required to store each 
  matrix once, along with an additional 200 MB per core used by each vanilla
  R session.
}
\subsection{Module Preservation Statistics:}{
 Module preservation is assessed through seven module preservation statistics,
 each of which captures a different aspect of a module's topology; \emph{i.e.}
 the structure of the relationships between its nodes \emph{(1,2)}. Below is
 a description of each statistic, what they seek to measure, and where their
 interpretation may be inappropriate. 
 
 The \emph{module coherence} (\code{'coherence'}), \emph{average node 
 contribution} (\code{'avg.contrib'}), and \emph{concordance of node 
 contribution} (\code{'cor.contrib'}) are all calculated from the data used 
 to infer the network (provided in the \code{'data'} argument). They are 
 calculated from the module's \emph{summary profile}. This is the eigenvector
 of the 1st principal component across all observations for every node
 composing the module. For gene coexpression modules this can be interpreted
 as a "summary expression profile". It is typically referred to as the
 "module eigengene" in the weighted gene coexpression network analysis
 literature \emph{(4)}.
 
 The \emph{module coherence} (\code{'coherence'}) quantifies the proportion 
 of module variance explained by the module's "summary profile". The higher
 this value, the more "coherent" the data is, \emph{i.e.} the more similar
 the observations are nodes for each sample. With the default alternate
 hypothesis, a small permutation \emph{P}-value indicates that the module is
 more coherent than expected by chance.
 
 The \emph{average node contribution} (\code{'avg.contrib'}) and 
 \emph{concordance of node contribution} (\code{'cor.contrib'}) are calculated 
 from the \emph{node contribution}, which quantifies how similar each node is 
 to the modules's \emph{summary profile}. It is calculated as the Pearson
 correlation coefficient between each node and the module summary profile. In
 the weighted gene coexpression network literature it is typically called the
 "module membership" \emph{(2)}.
 
 The \emph{average node contribution} (\code{'avg.contrib'}) quantifies how
 similar nodes are to the module summary profile in the test dataset. Nodes
 detract from this score where the sign of their node contribution flips 
 between the discovery and test datasets, \emph{e.g.} in the case of 
 differential gene expression across conditions. A high \emph{average node
 contribution} with a small permutation \emph{P}-value indicates that the
 module remains coherent in the test dataset, and that the nodes are acting
 together in a similar way.  
 
 The \emph{concordance of node contribution} (\code{'cor.contrib'}) measures 
 whether the relative rank of nodes (in terms of their node contribution) is 
 preserved across datasets. If a module is coherent enough that all nodes 
 contribute strongly, then this statistic will not be meaningful as its value
 will be heavily influenced by tiny variations in node rank. This can be
 assessed through visualisation of the module topology (see 
 \code{\link{plotContribution}}.) Similarly, a strong
 \code{'cor.contrib'} is unlikely to be meaningful if the
 \code{'avg.contrib'} is not significant.
 
 The \emph{concordance of correlation strucutre} (\code{'cor.cor'}) and 
 \emph{density of correlation structure} (\code{'avg.cor'}) are calculated 
 from the user-provided correlation structure between nodes (provided in the 
 \code{'correlation'} argument). This is referred to as "coexpression" when
 calculated on gene expression data.
 
 The \code{'avg.cor'} measures how strongly nodes within a module are 
 correlation on average in the test dataset. This average depends on the 
 correlation coefficients in the discovery dataset: the score is penalised 
 where correlation coefficients change in sign between datasets. A high 
 \code{'avg.cor'} with a small permutation \emph{P}-value indicates that the 
 module is (a) more strongly correlated than expected by chance for a module 
 of the same size, and (b) more consistently correlated with respect to the 
 discovery dataset than expected by chance.
 
 The \code{'cor.cor'} measures how similar the correlation coefficients are 
 across the two datasets. A high \code{'cor.cor'} with a small permutation 
 \emph{P}-value indicates that the correlation structure within a module is 
 more similar across datasets than expected by chance. If all nodes within a 
 module are very similarly correlated then this statistic will not be 
 meaningful, as its value will be heavily influenced by tiny, non-meaningful, 
 variations in correlation strength. This can be assessed through
 visualisation of the module topology (see \code{\link{plotCorrelation}}.)
 Similarly, a strong \code{'cor.cor'} is unlikely to be meaningful if the
 \code{'avg.cor'} is not significant.
 
 The \emph{average edge weight} (\code{'avg.weight'}) and \emph{concordance
 of weighted degree} (\code{'cor.degree'}) are both calculated from the 
 interaction network (provided as adjacency matrices to the \code{'network'}
 argument). 
 
 The \code{'avg.weight'} measures the average connection strength between 
 nodes in the test dataset. In the weighted gene coexpression network 
 literature this is typically called the "module density" \emph{(2)}. A high
 \code{'avg.weight'} with a small permutation \emph{P}-value indicates that
 the module is more strongly connected in the test dataset than expected by
 chance. 
 
 The \code{'cor.degree'} calculates whether the relative rank of each node's 
 \emph{weighted degree} is similar across datasets. The \emph{weighted
 degree} is calculated as the sum of a node's edge weights to all other nodes
 in the module. In the weighted gene coexpression network literature this is 
 typically called the "intramodular connectivity" \emph{(2)}. This statistic 
 will not be meaningful where all nodes are connected to each other with 
 similar strength, as its value will be heavily influenced by tiny,
 non-meaningful, variations in weighted degree. This can be assessed through
 visualisation of the module topology (see \code{\link{plotDegree}}.)
 
 Both the \code{'avg.weight'} and \code{'cor.degree'} assume edges are 
 weighted, and that the network is densely connected. Note that for sparse 
 networks, edges with zero weight are included when calculating both
 statistics. Only the magnitude of the weights, not their sign, contribute to
 the score. If the network is \emph{unweighted}, \emph{i.e.} edges indicate
 presence or absence of a relationship, then the \code{'avg.weight'} will be
 the proportion of the number of edges to the total number of possible edges
 while the \emph{weighted degree} simply becomes the \emph{degree}. A high
 \code{'avg.weight'} in this case measures how interconnected a module is in
 the test dataset. A high \emph{degree} indicates that a node is connected to
 many other nodes. The interpretation of the \code{'cor.degree'} remains
 unchanged between weighted and unweighted networks. If the network is
 directed the interpretation of the \code{'avg.weight'} remains unchanged,
 while the \emph{cor.degree} will measure the concordance of the node
 \emph{in-}degree in the test network. To measure the \emph{out-}degree
 instead, the adjacency matrices provided to the \code{'network'} argument
 should be transposed.
}
\subsection{Sparse data:}{
 Caution should be used when running \code{NetRep}
 on sparse data (\emph{i.e.} where there are many zero values in the data 
 used to infer the network). For this data, the \emph{average node contribution} 
 (\code{'avg.contrib'}), \emph{concordance of node contribution} 
 (\code{'cor.contrib'}), and \emph{module coherence} (\code{'coherence'})
 will all be systematically underestimated due to their reliance on the 
 Pearson correlation coefficient to calculate the \emph{node contribution}.
 
 Care should also be taken to use appropriate methods for inferring the
 correlation structure when the data is sparse for the same reason.
}
\subsection{Proportional data:}{
 Caution should be used when running \code{NetRep} on proportional data (
 \emph{i.e.} where observations across samples all sum to the same value, 
 \emph{e.g.} 1). For this data, the \emph{average node contribution} 
 (\code{'avg.contrib'}), \emph{concordance of node contribution} 
 (\code{'cor.contrib'}), and \emph{module coherence} (\code{'coherence'})
 will all be systematically overestimated due to their reliance on the 
 Pearson correlation coefficient to calculate the \emph{node contribution}.
 
 Care should also be taken to use appropriate methods for inferring the
 correlation structure from proportional data for the same reason.
}
\subsection{Hypothesis testing:}{
 Three alternative hypotheses are available. "greater", the default, tests
 whether each module preservation statistic is larger than expected by 
 chance. "lesser" tests whether each module preservation statistic is smaller
 than expected by chance, which may be useful for identifying modules that
 are extremely different in the \emph{test} dataset. "two.sided" can be used
 to test both alternate hypotheses.
 
 To determine whether a module preservation statistic deviates from chance, a
 permutation procedure is employed. Each statistic is calculated between the
 module in the \emph{discovery} dataset and \code{nPerm} random subsets of
 the same size in the \emph{test} dataset in order to assess the distribution
 of each statistic under the null hypothesis. Two models for the null 
 hypothesis are available. Under "overlap", the default, random sampling is
 performed only for the set of variables present in both the \emph{discovery} and
 \emph{test} datasets. Alternatively, the argument \code{null} can be set to
 "all", in which case random sampling is performed on all variables present in
 the \emph{test} dataset.
  
 The number of permutations required for any given significance threshold is 
 approximately 1 / the desired significance for one sided tests, and double 
 that for two-sided tests. This can be calculated with 
 \code{\link{requiredPerms}}. When \code{nPerm} is not specified, the number 
 of permutations is automatically calculated as the number required for a 
 Bonferroni corrected significance threshold adjusting for the total number 
 of tests for each statistic, i.e. the total number of modules to be analysed
 multiplied by the number of \emph{test} datasets each module is tested in. 
 Although assessing the replication of a small numberof modules calls for 
 very few permutations, we recommend using no fewer than 1,000 as fewer 
 permutations are unlikely to generate representative null distributions. 
 \strong{Note:} the assumption used by \code{\link{requiredPerms}} to 
 determine the correct number of permtutations breaks down when assessing the
 preservation of modules in a very small dataset (e.g. gene sets in a dataset
 with less than 100 genes total). However, the reported p-values will still
 be accurate (see \code{\link{permutationTest}}) \emph{(3)}.
}
}
\examples{
\dontrun{
# load in example data, correlation, and network matrices for a discovery and test dataset:
data("NetRep")

# Convert them to the 'bigMatrix' format:
discovery_data <- as.bigMatrix(discovery_data)
discovery_correlation <- as.bigMatrix(discovery_correlation)
discovery_network <- as.bigMatrix(discovery_network)
test_data <- as.bigMatrix(test_data)
test_correlation <- as.bigMatrix(test_correlation)
test_network <- as.bigMatrix(test_network)

# Set up input lists for each input matrix type across datasets:
data_list <- list(discovery=discovery_data, test=test_data)
correlation_list <- list(discovery=discovery_correlation, test=test_correlation)
network_list <- list(discovery=discovery_network, test=test_network)
labels_list <- list(discovery=module_labels)

# Assess module preservation.
preservation <- modulePreservation(
 data=data_list, correlation=correlation_list, network=network_list,
 moduleAssignments=labels_list, nPerm=1000, discovery="discovery", 
 test="test"
)

}

}
\references{
\enumerate{
    \item{
     Ritchie, S.C., \emph{et al.}, \emph{A scalable permutation approach
     reveals replication and preservation patterns of gene coexpression
     modules}. bioRxiv. 029553 (2015).
    }
    \item{
      Langfelder, P., Luo, R., Oldham, M. C. & Horvath, S. \emph{Is my
      network module preserved and reproducible?} PLoS Comput. Biol. 
      \strong{7}, e1001057 (2011). 
    }
    \item{
      Phipson, B. & Smyth, G. K. \emph{Permutation P-values should never be 
      zero: calculating exact P-values when permutations are randomly drawn.}
      Stat. Appl. Genet. Mol. Biol. \strong{9}, Article39 (2010). 
    }
    \item{
      Langfelder, P. & Horvath, S. \emph{WGCNA: an R package for weighted 
      correlation network analysis.} BMC Bioinformatics \strong{9}, 559 
      (2008).
    }
  }
}
\seealso{
Functions for: 
  \link[=bigMatrix]{bigMatrix objects},
  \link[=plotModule]{visualising network modules},
  \link[=networkProperties]{calculating module topology}, 
  \link[=permutationTest]{calculating permutation test P-values}, and 
  \link[=combineAnalyses]{splitting computation over multiple machines}.
}

