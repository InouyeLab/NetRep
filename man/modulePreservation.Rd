% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modulePreservation.R
\name{modulePreservation}
\alias{modulePreservation}
\title{Replication and preservation of network modules across datasets}
\usage{
modulePreservation(data = NULL, correlation, network, moduleAssignments,
  discovery = 1, test = 2, nCores = 1, nPerm, exclude, include,
  null = "overlap", alternative = "greater", statCorMethod = "pearson",
  simplify = TRUE, verbose = TRUE, keepNulls = FALSE)
}
\arguments{
\item{data}{a list of numeric matrices. Each entry of the list corresponds to
a dataset and contains the data used to infer the interaction network
between variables (e.g. genes). Expects matrix columns to correspond to
variables and matrix rows to correspond to samples.}

\item{correlation}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the correlation between 
each pair of variables in the dataset.}

\item{network}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the network edge weights 
between each pair of variables in the dataset.}

\item{moduleAssignments}{a vector containing the module each variable belongs
to in the discovery dataset. If there are multiple discovery datasets 
then this argument should be a list of such vectors.}

\item{discovery}{name or index denoting which the discovery dataset.}

\item{test}{name or index denoting the dataset to analyse the module in.}

\item{nCores}{number of cores to parallelise the permutation procedure over.}

\item{nPerm}{number of permutations to use. Can be specified as a vector if
a different number of permutations is desired for each discovery dataset.
If not specified, the number of permutations will be automatically 
determined (see details).}

\item{exclude}{an optional vector of modules to exclude from the analysis. If
there are multiple discovery datasets a list of vectors may be provided.}

\item{include}{an optional vector of modules to include in the
analysis. If there are multiple discovery datasets a list of vectors may be
provided.}

\item{null}{variables to include when generating the null distributions. 
Must be either "overlap" or "all" (see details).}

\item{alternative}{The type of module preservation test to perform. Must be 
one of "greater" (default), "less" or "two.sided" (see details).}

\item{simplify}{logical; if \code{TRUE}, simplify the structure of the output
list if possible (see Return Value).}

\item{verbose}{logical; should progress be reported? Default is \code{TRUE}.}

\item{keepNulls}{logical; if \code{TRUE}, the null distributions are returned
as part of the output.}
}
\value{
The returned data structure is organised as a nested list of lists, which 
 should be accessed as \code{results[[discovery]][[test]]}. If
 \code{simplify} is set to \code{TRUE}, then this structure will be 
 simplified as much as possible depending on the combination of dataset 
 comparisons that have been performed. For each dataset-comparison a list of
 the following objects are returned:
 \itemize{
   \item{\code{observed}:}{
     A matrix of the observed values for the module preservation statistics.
     Rows correspond to modules, and columns to the module preservation
     statistics.
   }
   \item{\code{nulls}:}{
     A three dimensional array containing the values of the module 
     preservation statistics evaluated on random permutation of module 
     assignment in the test network. Rows correspond to modules, columns to
     the module preservation statistics, and the third dimension to the 
     permutations.
   }
   \item{\code{p.values}:}{
     A matrix of p-values for the \code{observed} module preservation 
     statistics as evaluated through a permutation test using the 
     corresponding values in \code{nulls}.
   }
   \item{\code{nVarsPresent}:}{
     A vector containing the number of genes that are present in the test
     dataset for each module.
   }
   \item{\code{propVarsPresent}:}{
     A vector containing the proportion of genes present in the test dataset
     for each module. Modules where this is less than 1 should be 
     investigated further before making judgements about preservation to 
     ensure that the missing genes are not the most connected ones.
   }
   \item{\code{contingency}:}{ 
     If \code{moduleAssignments} are present for both the \emph{discovery}
     and \emph{test} datasets, then a contigency table showing the overlap
     between modules across datasets is returned. Rows correspond to modules
     in the \emph{discovery} dataset, columns to modules in the \emph{test}
     dataset.
   }
 }
}
\details{
\subsection{Input data structure:}{
  The topological properties used to assess module preservation are designed 
  for networks constructed using Weighted Gene Coexpression Network Analysis 
  (\pkg{\link[WGCNA]{WGCNA}}, \emph{(3)}). These are calculated from the gene expression
  for each dataset, the pairwise correlation between genes (coexpression) for
  each dataset, and the pairwise gene adjacencies (network) for each
  dataset. The network is typically the absolute value of the correlation
  raised to a power to penalise weak correlations \emph{(3)}. Module
  preservation can also be assessed on networks without the gene expression
  data, but only a limited subset of the statistics will be calculated.
  Network modules are usually clusters of tightly coexpressed genes
  \emph{(3)}, but the procedure is also useful for assessing known gene sets,
  i.e. pathways across conditions or tissues \emph{(1)}.
  
  The arguments \code{data}, \code{correlation}, and 
  \code{network} each expect a \code{\link{list}} where each element 
  contains the matrix data for each respective dataset. This matrix data 
  should be stored as a 'bigMatrix' object (see 
  \link[=bigMatrix-get]{converting matrix data to 'bigMatrix' data}).
  Similarly, the \code{moduleAssignments} argument expects a list of named
  vectors, which contain the the module assignments for each gene in the
  respective dataset. List elements corresponding to datasets where module
  discovery has not been performed should contain \code{NULL}, unless the
  datasets are named throughout the function arguments. I.e. where the
  \code{\link{names}} of \code{data}, \code{correlation}, and
  \code{network} correspond to the names of each dataset of interest, the
  names of the \code{discovery} dataset can be used to look up the respective
  module assignments in the \code{moduleAssignments} list. If module
  discovery has only been performed in one dataset, then the
  \code{moduleAssignments} will also accept a named vector.
  
  The \code{discovery} arguments specifies which dataset the \code{modules} 
  of interest were discovered in, and the \code{test} argument specifies 
  which dataset to test the replication/preservation of each module.
}
\subsection{'bigMatrix' vs. 'matrix' input data:}{
  Although the function expects \code{\link[=bigMatrix-class]{bigMatrix}}
  data, regular 'matrix' objects are also accepted. In this case, the
  'matrix' data is temporarily converted to 'bigMatrix' by the function. This
  conversion process involves writing out each matrix as a binary file on
  disk, which can take a long time for large datasets. It is strongly
  recommended for the user to store their data as 'bigMatrix' objects, as the
  \link{networkProperties} function, \link[=plotModule]{plotting} 
  \link[=plotTopology]{functions}, \link[=geneOrder]{gene} and 
  \link[=sampleOrder]{sample} ordering also expect 'bigMatrix' objects. 
  Further, 'bigMatrix' objects have a number of benefits, including 
  instantaneous load time from any future R session, and parallel access from
  mutliple independent R sessions. Methods are provided for 
  \link[=bigMatrix-get]{converting to, loading in}, and 
  \link[=bigMatrix-out]{writing out} 'bigMatrix' objects.
}
\subsection{Memory usage:}{
  A trade off has been made between memory usage and computation time. 
  'modulePreservation' has a large memory overhead as it requires 
  pre-computed correlation and network matrices for each dataset. However,
  these are stored in shared memory, which means that each parallel process
  can independently access this memory. There is very little memory overhead
  for each additional core. 
  
  Although this also means that the matrices can be larger than the available
  RAM, in practice we find that this slows down the procedure by several 
  orders of magnitude. For optimal performance, there should be sufficient
  memory to load in each gene expression, correlation, and network matrix
  for each dataset. Note: most of this memory is cached; matrices are only 
  loaded into RAM when needed (i.e. for the dataset pair for a comparison),
  so the physical amount of RAM used will be much lower.
}
\subsection{Module Preservation Statistics:}{
 Module preservation is assessed through seven statistics \emph{(1)}:
 \enumerate{
   \item{\code{mean.adj}:}{
     The mean adjacency, or module density, measures how densely connected a 
     module is in the \emph{test} dataset.
   }
   \item{\code{pve}:}{
     Short for "the proportion of variance explained in the underlying gene
     expression data for the module by its summary expression profile in the
     \emph{test} dataset". The summary expression profile is calculated as
     the first eigenvector from a principal component analysis on the
     module's (scaled) gene expression data. The summary expression profile
     is commonly referred to as the "module eigengene (ME)", and abbreviated
     as \code{propVarExpl} in \emph{(1)}.
   }
   \item{\code{cor.coexp}:}{
     The correlation of correlation patterns for a module across the
     \emph{discovery} and \emph{test} datasets. It is also referred to as
     the "correlation of correlation" and abbreviated as \code{cor.cor} in 
     \emph{(1)}.
   }
   \item{\code{cor.kIM}:}{
     The correlation of intramodular connectivity across the \emph{discovery}
     and \emph{test} datasets. Intramodular connectivity is quantified as the
     sum of adjacency for a gene to all other genes in the module.
   }
   \item{\code{cor.MM}:}{
     The correlation of intramodular module membership across the
     \emph{discovery} and \emph{test} datasets. Module membership denotes the
     correlation between each gene and the summary expression profile for
     that module in the given dataset. The statistic is abbreviated as
     \code{cor.kME} in \emph{(1)}.
   }
   \item{\code{mean.coexp}:}{
     The mean sign-aware correlation measures the average correlation for a
     module in the \emph{test} dataset, multiplied by the sign of the 
     correlation in the \emph{discovery} dataset. It assesses how strongly 
     correlated the genes are in the test dataset (in either direction), 
     penalising the module if any gene pairs whose correlation flips between
     datasets. It is also referred to as the "mean sign-aware correlation"
     and abbreviated as \code{mean.cor} in \emph{(1)}.
   }
   \item{\code{mean.MM}:}{
     The mean sign-aware module membership measures the average module 
     membership in the \emph{test} dataset, multiplied by the sign of the 
     module membership in the \emph{discovery} dataset. It measures how 
     coherent the gene expression is in the \emph{test} dataset, penalising 
     the module if any genes are differentially expressed compared to the 
     module in one, but not both \emph{discovery} and \emph{test} datasets. 
     It is also abbreivated as \code{mean.kME} in \emph{(1)}.
   }
 }
}
\subsection{Hypothesis testing:}{
 Three alternative hypotheses are available. "greater", the default, tests
 whether each module preservation statistic is larger than expected by 
 chance. "lesser" tests whether each module preservation statistic is smaller
 than expected by chance, which may be useful for identifying modules that
 are extremely different in the \emph{test} dataset. "two.sided" can be used
 to test both alternate hypotheses.
 
 To determine whether a module preservation statistic deviates from chance, a
 permutation procedure is employed. Each statistic is calculated between the
 module in the \emph{discovery} dataset and \code{nPerm} random gene sets of
 the same size in the \emph{test} dataset in order to assess the distribution
 of each statistic under the null hypothesis. Two models for the null 
 hypothesis are available. Under "overlap", the default, random sampling is
 performed only for the set of genes present in both the \emph{discovery} and
 \emph{test} datasets. Alternative, the argument \code{null} can be set to
 "all", in which case random sampling is performed on all genes present in
 the \emph{test} dataset. The latter may be suitable when assessing module
 replication across species.
  
 The number of permutations required for any given significance threshold is 
 approximately 1 / the desired significance for one sided tests, and double 
 that for two-sided tests. This can be calculated with 
 \code{\link{requiredPerms}}. When \code{nPerm} is not specified, the number 
 of permutations is automatically calculated as the number required for a 
 Bonferroni corrected significance threshold adjusting for the total number 
 of tests for each statistic, i.e. the total number of modules to be analysed
 multiplied by the number of \emph{test} datasets each module is tested in. 
 Although assessing the replication of a small numberof modules calls for 
 very few permutations, we recommend using no fewer than 1,000 as fewer 
 permutations are unlikely to generate representative null distributions. 
 \strong{Note:} the assumption used by \code{\link{requiredPerms}} to 
 determine the correct number of permtutations breaks down when assessing the
 preservation of modules in a very small dataset (e.g. gene sets in a dataset
 with less than 100 genes total). However, the reported p-values will still
 be accurate (see \code{\link{perm.test}}) \emph{(2)}.
}
}
\examples{
\dontrun{
set.seed(1)

## Example 1: Assess replication of all modules from one 
## cohort in an independent dataset

# First we need some example data
geA <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geA) <- paste0("Gene_", 1:100)
rownames(geA) <- paste0("CohortA_", 1:50)
coexpA <- cor(geA) # correlation
adjA <- abs(coexpA)^5 # adjacency
moduleAssignments <- sample(1:7, size=100, replace=TRUE)
names(moduleAssignments) <- paste0("Gene_", 1:100)

geB <- matrix(rnorm(70*100), ncol=100) # gene expression
colnames(geB) <- paste0("Gene_", 1:100) 
rownames(geB) <- paste0("CohortB_", 1:70)
coexpB <- cor(geB) # correlation
adjB <- abs(coexpB)^6 # adjacency

# Now format the data for input to modulePreservation
data <- list(
  cohortA=as.bigMatrix(geA, "geA_bm"),
  cohortB=as.bigMatrix(geA, "geA_bm")    
)
correlation <- list(
  cohortA=as.bigMatrix(coexpA, "coexpA_bm"),
  cohortB=as.bigMatrix(coexpB, "coexpB_bm")
)
adjacency <- list(
  cohortA=as.bigMatrix(adjA, "adjA_bm"),
  cohortB=as.bigMatrix(adjB, "adjB_bm")
)

# Assess module preservation, using two cores
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments, 
  nCores=2
)

## Example 2: assess replication of two disease-associated modules
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, include = c("4", "7")
)

## Example 3: exclude a module from the analysis
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, exclude = "0"
)

## Example 4: assess preservation of modules across multiple
## tissues
geAdipose <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geAdipose) <- paste0("Gene_", 1:100)
rownames(geAdipose) <- paste0("Sample_", 1:50)
coexpAdipose <- cor(geAdipose) # correlation
adjAdipose <- abs(coexpAdipose)^5 # adjacency
adiposeModules <- sample(0:7, size=100, replace=TRUE)
names(adiposeModules) <- paste0("Gene_", 1:100)

geLiver <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geLiver) <- paste0("Gene_", 1:100)
rownames(geLiver) <- paste0("Sample_", 1:50)
coexpLiver <- cor(geLiver) # correlation
adjLiver <- abs(coexpLiver)^6 # adjacency
liverModules <- sample(0:12, size=100, replace=TRUE)
names(liverModules) <- paste0("Gene_", 1:100)

geHeart <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geHeart) <- paste0("Gene_", 1:100)
rownames(geHeart) <- paste0("Sample_", 1:50)
coexpHeart <- cor(geHeart) # correlation
adjHeart <- abs(coexpHeart)^4 # adjacency
heartModules <- sample(0:5, size=100, replace=TRUE)
names(heartModules) <- paste0("Gene_", 1:100)

# Now format the data for input to modulePreservation
data <- list(
  adipose=as.bigMatrix(geAdipose, "geAdipose_bm"),
  liver=as.bigMatrix(geLiver, "geLiver_bm"),  
  heart=as.bigMatrix(geHeart, "geHeart_bm") 
)
correlation <- list(
  adipose=as.bigMatrix(coexpAdipose, "coexpAdipose_bm"),
  liver=as.bigMatrix(coexpLiver, "coexpLiver_bm"),  
  heart=as.bigMatrix(coexpHeart, "coexpHeart_bm") 
)
adjacency <- list(
  adipose=as.bigMatrix(adjAdipose, "adjAdipose_bm"),
  liver=as.bigMatrix(adjLiver, "adjLiver_bm"),  
  heart=as.bigMatrix(adjHeart, "adjHeart_bm") 
)
moduleAssignments <- list(
  adipose=adiposeModules, liver=liverModules, heart=heartModules
)

# Assess the preservation of each module in each non-discovery
# tissue.
preservation <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, discovery=c("adipose", "liver", "heart"), 
  test=c("adipose", "liver", "heart")
)

# Remove bigMatrix files used in examples
unlink("*_bm*")
}

}
\references{
\enumerate{
    \item{
      Langfelder, P., Luo, R., Oldham, M. C. & Horvath, S. \emph{Is my
      network module preserved and reproducible?} PLoS Comput. Biol. 
      \strong{7}, e1001057 (2011). 
    }
    \item{
      Phipson, B. & Smyth, G. K. \emph{Permutation P-values should never be 
      zero: calculating exact P-values when permutations are randomly drawn.}
      Stat. Appl. Genet. Mol. Biol. \strong{9}, Article39 (2010). 
    }
    \item{
      Langfelder, P. & Horvath, S. \emph{WGCNA: an R package for weighted 
      correlation network analysis.} BMC Bioinformatics \strong{9}, 559 
      (2008).
    }
  }
}

