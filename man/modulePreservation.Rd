% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modulePreservation.R
\name{modulePreservation}
\alias{modulePreservation}
\title{Replication and preservation of network modules across datasets}
\usage{
modulePreservation(data = NULL, correlation, network, moduleAssignments,
  modules = NULL, backgroundLabel = "0", discovery = 1, test = 2,
  selfPreservation = FALSE, nCores = 1, nPerm = NULL, null = "overlap",
  alternative = "greater", statCorMethod = "pearson", simplify = TRUE,
  verbose = TRUE, keepNulls = FALSE)
}
\arguments{
\item{data}{a list of numeric matrices. Each entry of the list corresponds to
a dataset and contains the data used to infer the interaction network
between variables (e.g. genes). Expects matrix columns to correspond to
variables and matrix rows to correspond to samples.}

\item{correlation}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the correlation between 
each pair of variables in the dataset.}

\item{network}{a list of matrices. Each entry of the list corresponds to a 
dataset and contains an \eqn{n * n} matrix of the network edge weights 
between each pair of variables in the dataset.}

\item{moduleAssignments}{a vector containing the module each variable belongs
to in the discovery dataset. If there are multiple discovery datasets 
then this argument should be a list of such vectors.}

\item{modules}{a list of vectors, one for each \code{discovery} dataset, 
of modules to perform the analysis on. The default is to analyse all modules
with the exception of those specified in \code{backgroundLabel}.}

\item{backgroundLabel}{a single label that nodes that do not belong to any
module are assigned. The default is "0".}

\item{discovery}{a vector of names or indices denoting the discovery dataset(s).}

\item{test}{a list of vectors of names or indices denoting the test datasets
for each \code{discovery} dataset. Alternatively can be provided as vector
if the test datasets are the same for all 'discovery' datasets (e.g. for 
performing a pairwise comparison).}

\item{selfPreservation}{logical; if \code{FALSE} (default) then module 
preservation analysis will not be performed where the \code{discovery} and
\code{test} datasets are the same.}

\item{nCores}{number of cores to parallelise the permutation procedure over.
Ignored if the user has already registered a parallel backend.}

\item{nPerm}{number of permutations to use. If not specified, the number of 
permutations will be automaticallydetermined (see details).}

\item{null}{variables to include when generating the null distributions. 
Must be either "overlap" or "all" (see details).}

\item{alternative}{The type of module preservation test to perform. Must be 
one of "greater" (default), "less" or "two.sided" (see details).}

\item{statCorMethod}{character vector indicating method to use when calculating 
the correlation based statistics (see details). Must be one of "pearson", 
"spearman", or "kendall". If the WGCNA package is installed then "bicor" 
may also be specified as an option (see \code{\link[WGCNA]{bicor}}).}

\item{simplify}{logical; if \code{TRUE}, simplify the structure of the output
list if possible (see Return Value).}

\item{verbose}{logical; should progress be reported? Default is \code{TRUE}.}

\item{keepNulls}{logical; if \code{TRUE}, the null distributions are returned
as part of the output.}
}
\value{
A nested list structure. At the top level, the list has one element per 
 \code{'discovery'} dataset. Each of these elements is a list that has one
 element per \code{'test'} dataset analysed for that \code{'discovery'} 
 dataset. Each of these elements is also a list, containing the following
 objects:
 \itemize{
   \item{\code{observed}:}{
     A matrix of the observed values for the module preservation statistics.
     Rows correspond to modules, and columns to the module preservation
     statistics.
   }
   \item{\code{nulls}:}{
     A three dimensional array containing the values of the module 
     preservation statistics evaluated on random permutation of module 
     assignment in the test network. Rows correspond to modules, columns to
     the module preservation statistics, and the third dimension to the 
     permutations.
   }
   \item{\code{p.values}:}{
     A matrix of p-values for the \code{observed} module preservation 
     statistics as evaluated through a permutation test using the 
     corresponding values in \code{nulls}.
   }
   \item{\code{nVarsPresent}:}{
     A vector containing the number of variables that are present in the test
     dataset for each module.
   }
   \item{\code{propVarsPresent}:}{
     A vector containing the proportion of variables present in the test dataset
     for each module. Modules where this is less than 1 should be 
     investigated further before making judgements about preservation to 
     ensure that the missing variables are not the most connected ones.
   }
   \item{\code{contingency}:}{ 
     If \code{moduleAssignments} are present for both the \emph{discovery}
     and \emph{test} datasets, then a contingency table showing the overlap
     between modules across datasets is returned. Rows correspond to modules
     in the \emph{discovery} dataset, columns to modules in the \emph{test}
     dataset.
   }
 }
 
 For example, \code{results[[1]][[2]][["p.values"]]} is the matrix of 
 module preservation p-values when assessing the preservation of modules from
 dataset 1 in dataset 2. If \code{simplify = TRUE} then the list structure 
 will be simplified where possible.
}
\description{
Quantify the preservation of network modules (sub-graphs) in an independent
dataset through permutation testing on module topology. Seven network
statistics (see details) are calculated for each module and then tested by
comparing to distributions generated from their calculation on random subsets
in the test dataset.
}
\details{
\subsection{Input data structure:}{
  This function allows for input data formatted in a number of ways. Where 
  there are multiple datasets of interest (e.g. multiple tissues, locations, 
  or a discovery dataset and an independent test dataset) the arguments 
  \code{data}, \code{correlation}, and \code{network} should be
  \code{\link[=list]{lists}} where each element contains the matrix data for 
  each respective dataset. Alternatively, if only one dataset is of interest, 
  the \code{data}, \code{correlation}, and \code{network} arguments
  will also each accept a 'matrix' object.
  
  Similarly, the \code{moduleAssignments} argument expects a list of named
  vectors, which denote the module each variable belongs to in the discovery
  dataset. If module discovery has only been performed in one dataset, then 
  the \code{moduleAssignments} argument will also accept a named vector.
  
  The \code{discovery} arguments specifies which dataset the \code{modules} 
  of interest were discovered in, and the \code{test} argument specifies 
  which dataset to calculate the network properties in. These arguments are
  ignored if data is provided for only one dataset.
}
\subsection{'bigMatrix' vs. 'matrix' input data:}{
  Although the function expects \code{\link[=bigMatrix-class]{bigMatrix}} 
  data, regular 'matrix' objects are also accepted. In this case, the 
  'matrix' data is temporarily converted to 'bigMatrix' by the function. This
  conversion process involves writing out each matrix as a binary file on 
  disk, which can take a long time for large datasets. It is strongly 
  recommended for the user to store their data as 'bigMatrix' objects, as the
  \link{modulePreservation} function, \link[=plotModule]{plotting} 
  \link[=plotTopology]{functions}, \link[=nodeOrder]{node} and 
  \link[=sampleOrder]{sample} ordering functions also expect 'bigMatrix'
  objects. Further, 'bigMatrix' objects have a number of benefits, including 
  instantaneous load time from any future R session, and parallel access from
  mutliple independent R sessions. Methods are provided for 
  \link[=bigMatrix-get]{converting to, loading in}, and 
  \link[=bigMatrix-out]{writing out} 'bigMatrix' objects.
}
\subsection{Memory usage:}{
  A trade off has been made between memory usage and computation time. 
  'modulePreservation' has a large memory overhead as it requires 
  pre-computed correlation and network matrices for each dataset. However,
  these are stored in shared memory, which means that each parallel process
  can independently access this memory. There is very little memory overhead
  for each additional core. 
  
  Although this also means that the matrices can be larger than the available
  RAM, in practice we find that this slows down the procedure by several 
  orders of magnitude. For optimal performance, there should be sufficient
  memory to load in each gene expression, correlation, and network matrix
  for each dataset. Note: most of this memory is cached; matrices are only 
  loaded into RAM when needed (i.e. for the dataset pair for a comparison),
  so the physical amount of RAM used will be much lower.
}
\subsection{Module Preservation Statistics:}{
 Module preservation is assessed through seven module preservation statistics,
 each of which captures a different aspect of a module's topology; \emph{i.e.}
 the structure of the relationships between its nodes \emph{(1,2)}. Below is
 a description of each statistic, what they seek to measure, and where their
 interpretation may be inappropriate. 
 
 The \emph{module coherence} (\code{'coherence'}), \emph{average node 
 contribution} (\code{'avg.contrib'}), and \emph{concordance of node 
 contribution} (\code{'cor.contrib'}) are all calculated from the data used 
 to infer the network (provided in the \code{'data'} argument). They are 
 calculated from the module's \emph{summary profile}. This is the eigenvector
 of the 1st principal component across all observations for every node
 composing the module. For gene coexpression modules this can be interpreted
 as a "summary expression profile". It is typically referred to as the
 "module eigengene" in the weighted gene coexpression network analysis
 literature \emph{(4)}.
 
 The \emph{module coherence} (\code{'coherence'}) quantifies the proportion 
 of module variance explained by the module's "summary profile". The higher
 this value, the more "coherent" the data is, \emph{i.e.} the more similar
 the observations are nodes for each sample. With the default alternate
 hypothesis, a small permutation \emph{P}-value indicates that the module is
 more coherent than expected by chance.
 
 The \emph{average node contribution} (\code{'avg.contrib'}) and 
 \emph{concordance of node contribution} (\code{'cor.contrib'}) are calculated 
 from the \emph{node contribution}, which quantifies how similar each node is 
 to the modules's \emph{summary profile}. It is calculated as the Pearson
 correlation coefficient between each node and the module summary profile. In
 the weighted gene coexpression network literature it is typically called the
 "module membership" \emph{(2)}.
 
 The \emph{average node contribution} (\code{'avg.contrib'}) quantifies how
 similar nodes are to the module summary profile in the test dataset. Nodes
 detract from this score where the sign of their node contribution flips 
 between the discovery and test datasets, \emph{e.g.} in the case of 
 differential gene expression across conditions. A high \emph{average node
 contribution} with a small permutation \emph{P}-value indicates that the
 module remains coherent in the test dataset, and that the nodes are acting
 together in a similar way.  
 
 The \emph{concordance of node contribution} (\code{'cor.contrib'}) measures 
 whether the relative rank of nodes (in terms of their node contribution) is 
 preserved across datasets. If a module is coherent enough that all nodes 
 contribute strongly, then this statistic will not be meaningful as its value
 will be heavily influenced by tiny variations in node rank. This can be
 assessed through visualisation of the module topology (see 
 \code{\link{plotContribution}}.) Similarly, a strong
 \code{'cor.contrib'} is unlikely to be meaningful if the
 \code{'avg.contrib'} is not significant.
 
 The \emph{concordance of correlation strucutre} (\code{'cor.cor'}) and 
 \emph{density of correlation structure} (\code{'avg.cor'}) are calculated 
 from the user-provided correlation structure between nodes (provided in the 
 \code{'correlation'} argument). This is referred to as "coexpression" when
 calculated on gene expression data.
 
 The \code{'avg.cor'} measures how strongly nodes within a module are 
 correlation on average in the test dataset. This average depends on the 
 correlation coefficients in the discovery dataset: the score is penalised 
 where correlation coefficients change in sign between datasets. A high 
 \code{'avg.cor'} with a small permutation \emph{P}-value indicates that the 
 module is (a) more strongly correlated than expected by chance for a module 
 of the same size, and (b) more consistently correlated with respect to the 
 discovery dataset than expected by chance.
 
 The \code{'cor.cor'} measures how similar the correlation coefficients are 
 across the two datasets. A high \code{'cor.cor'} with a small permutation 
 \emph{P}-value indicates that the correlation structure within a module is 
 more similar across datasets than expected by chance. If all nodes within a 
 module are very similarly correlated then this statistic will not be 
 meaningful, as its value will be heavily influenced by tiny, non-meaningful, 
 variations in correlation strength. This can be assessed through
 visualisation of the module topology (see \code{\link{plotCorrelation}}.)
 Similarly, a strong \code{'cor.cor'} is unlikely to be meaningful if the
 \code{'avg.cor'} is not significant.
 
 The \emph{average edge weight} (\code{'avg.weight'}) and \emph{concordance
 of weighted degree} (\code{'cor.degree'}) are both calculated from the 
 interaction network (provided as adjacency matrices to the \code{'network'}
 argument). 
 
 The \code{'avg.weight'} measures the average connection strength between 
 nodes in the test dataset. In the weighted gene coexpression network 
 literature this is typically called the "module density" \emph{(2)}. A high
 \code{'avg.weight'} with a small permutation \emph{P}-value indicates that
 the module is more strongly connected in the test dataset than expected by
 chance. 
 
 The \code{'cor.degree'} calculates whether the relative rank of each node's 
 \emph{weighted degree} is similar across datasets. The \emph{weighted
 degree} is calculated as the sum of a node's edge weights to all other nodes
 in the module. In the weighted gene coexpression network literature this is 
 typically called the "intramodular connectivity" \emph{(2)}. This statistic 
 will not be meaningful where all nodes are connected to each other with 
 similar strength, as its value will be heavily influenced by tiny,
 non-meaningful, variations in weighted degree. This can be assessed through
 visualisation of the module topology (see \code{\link{plotDegree}}.)
 
 Both the \code{'avg.weight'} and \code{'cor.degree'} assume edges are 
 weighted, and that the network is densely connected. Note that for sparse 
 networks, edges with zero weight are included when calculating both
 statistics. Only the magnitude of the weights, not their sign, contribute to
 the score. If the network is \emph{unweighted}, \emph{i.e.} edges indicate
 presence or absence of a relationship, then the \code{'avg.weight'} will be
 the proportion of the number of edges to the total number of possible edges
 while the \emph{weighted degree} simply becomes the \emph{degree}. A high
 \code{'avg.weight'} in this case measures how interconnected a module is in
 the test dataset. A high \emph{degree} indicates that a node is connected to
 many other nodes. The interpretation of the \code{'cor.degree'} remains
 unchanged between weighted and unweighted networks. If the network is
 directed the interpretation of the \code{'avg.weight'} remains unchanged,
 while the \emph{cor.degree} will measure the concordance of the node
 \emph{in-}degree in the test network. To measure the \emph{out-}degree
 instead, the adjacency matrices provided to the \code{'network'} argument
 should be transposed.
}
\subsection{Sparse data:}{
 Caution should be used when running \code{NetRep}
 on sparse data (\emph{i.e.} where there are many zero values in the data 
 used to infer the network). For this data, the \emph{average node contribution} 
 (\code{'avg.contrib'}), \emph{concordance of node contribution} 
 (\code{'cor.contrib'}), and \emph{module coherence} (\code{'coherence'})
 will all be systematically underestimated due to their reliance on the 
 Pearson correlation coefficient to calculate the \emph{node contribution}.
 
 Care should also be taken to use appropriate methods for inferring the
 correlation structure when the data is sparse for the same reason.
}
\subsection{Proportional data:}{
 Caution should be used when running \code{NetRep} on proportional data (
 \emph{i.e.} where observations across samples all sum to the same value, 
 \emph{e.g.} 1). For this data, the \emph{average node contribution} 
 (\code{'avg.contrib'}), \emph{concordance of node contribution} 
 (\code{'cor.contrib'}), and \emph{module coherence} (\code{'coherence'})
 will all be systematically overestimated due to their reliance on the 
 Pearson correlation coefficient to calculate the \emph{node contribution}.
 
 Care should also be taken to use appropriate methods for inferring the
 correlation structure from proportional data for the same reason.
}
\subsection{Hypothesis testing:}{
 Three alternative hypotheses are available. "greater", the default, tests
 whether each module preservation statistic is larger than expected by 
 chance. "lesser" tests whether each module preservation statistic is smaller
 than expected by chance, which may be useful for identifying modules that
 are extremely different in the \emph{test} dataset. "two.sided" can be used
 to test both alternate hypotheses.
 
 To determine whether a module preservation statistic deviates from chance, a
 permutation procedure is employed. Each statistic is calculated between the
 module in the \emph{discovery} dataset and \code{nPerm} random subsets of
 the same size in the \emph{test} dataset in order to assess the distribution
 of each statistic under the null hypothesis. Two models for the null 
 hypothesis are available. Under "overlap", the default, random sampling is
 performed only for the set of variables present in both the \emph{discovery} and
 \emph{test} datasets. Alternatively, the argument \code{null} can be set to
 "all", in which case random sampling is performed on all variables present in
 the \emph{test} dataset.
  
 The number of permutations required for any given significance threshold is 
 approximately 1 / the desired significance for one sided tests, and double 
 that for two-sided tests. This can be calculated with 
 \code{\link{requiredPerms}}. When \code{nPerm} is not specified, the number 
 of permutations is automatically calculated as the number required for a 
 Bonferroni corrected significance threshold adjusting for the total number 
 of tests for each statistic, i.e. the total number of modules to be analysed
 multiplied by the number of \emph{test} datasets each module is tested in. 
 Although assessing the replication of a small numberof modules calls for 
 very few permutations, we recommend using no fewer than 1,000 as fewer 
 permutations are unlikely to generate representative null distributions. 
 \strong{Note:} the assumption used by \code{\link{requiredPerms}} to 
 determine the correct number of permtutations breaks down when assessing the
 preservation of modules in a very small dataset (e.g. gene sets in a dataset
 with less than 100 genes total). However, the reported p-values will still
 be accurate (see \code{\link{perm.test}}) \emph{(3)}.
}
}
\examples{
\dontrun{
set.seed(1)

## Example 1: Assess replication of all modules from one 
## cohort in an independent dataset

# First we need some example data
geA <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geA) <- paste0("Gene_", 1:100)
rownames(geA) <- paste0("CohortA_", 1:50)
coexpA <- cor(geA) # correlation
adjA <- abs(coexpA)^5 # adjacency
moduleAssignments <- sample(1:7, size=100, replace=TRUE)
names(moduleAssignments) <- paste0("Gene_", 1:100)

geB <- matrix(rnorm(70*100), ncol=100) # gene expression
colnames(geB) <- paste0("Gene_", 1:100) 
rownames(geB) <- paste0("CohortB_", 1:70)
coexpB <- cor(geB) # correlation
adjB <- abs(coexpB)^6 # adjacency

# Now format the data for input to modulePreservation
data <- list(
  cohortA=as.bigMatrix(geA, "geA_bm"),
  cohortB=as.bigMatrix(geA, "geB_bm")    
)
correlation <- list(
  cohortA=as.bigMatrix(coexpA, "coexpA_bm"),
  cohortB=as.bigMatrix(coexpB, "coexpB_bm")
)
adjacency <- list(
  cohortA=as.bigMatrix(adjA, "adjA_bm"),
  cohortB=as.bigMatrix(adjB, "adjB_bm")
)

# Assess module preservation, using two cores
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments, 
  nCores=2
)

## Example 2: assess replication of two disease-associated modules
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, include = c("4", "7")
)

## Example 3: exclude a module from the analysis
replication <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, exclude = "0"
)

## Example 4: assess preservation of modules across multiple
## tissues
geAdipose <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geAdipose) <- paste0("Gene_", 1:100)
rownames(geAdipose) <- paste0("Sample_", 1:50)
coexpAdipose <- cor(geAdipose) # correlation
adjAdipose <- abs(coexpAdipose)^5 # adjacency
adiposeModules <- sample(0:7, size=100, replace=TRUE)
names(adiposeModules) <- paste0("Gene_", 1:100)

geLiver <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geLiver) <- paste0("Gene_", 1:100)
rownames(geLiver) <- paste0("Sample_", 1:50)
coexpLiver <- cor(geLiver) # correlation
adjLiver <- abs(coexpLiver)^6 # adjacency
liverModules <- sample(0:12, size=100, replace=TRUE)
names(liverModules) <- paste0("Gene_", 1:100)

geHeart <- matrix(rnorm(50*100), ncol=100) # gene expression
colnames(geHeart) <- paste0("Gene_", 1:100)
rownames(geHeart) <- paste0("Sample_", 1:50)
coexpHeart <- cor(geHeart) # correlation
adjHeart <- abs(coexpHeart)^4 # adjacency
heartModules <- sample(0:5, size=100, replace=TRUE)
names(heartModules) <- paste0("Gene_", 1:100)

# Now format the data for input to modulePreservation
data <- list(
  adipose=as.bigMatrix(geAdipose, "geAdipose_bm"),
  liver=as.bigMatrix(geLiver, "geLiver_bm"),  
  heart=as.bigMatrix(geHeart, "geHeart_bm") 
)
correlation <- list(
  adipose=as.bigMatrix(coexpAdipose, "coexpAdipose_bm"),
  liver=as.bigMatrix(coexpLiver, "coexpLiver_bm"),  
  heart=as.bigMatrix(coexpHeart, "coexpHeart_bm") 
)
adjacency <- list(
  adipose=as.bigMatrix(adjAdipose, "adjAdipose_bm"),
  liver=as.bigMatrix(adjLiver, "adjLiver_bm"),  
  heart=as.bigMatrix(adjHeart, "adjHeart_bm") 
)
moduleAssignments <- list(
  adipose=adiposeModules, liver=liverModules, heart=heartModules
)

# Assess the preservation of each module in each non-discovery
# tissue.
preservation <- modulePreservation(
  data, correlation, adjacency, moduleAssignments,
  nCores=2, discovery=c("adipose", "liver", "heart"), 
  test=c("adipose", "liver", "heart")
)

# Remove bigMatrix files used in examples
unlink("*_bm*")
}

}
\references{
\enumerate{
    \item{
     Ritchie, S.C., et al., \emph{A scalable permutation approach reveals 
     replication and preservation patterns of network modules.} Cell Systems.
     \emph{in review} (2016).
    }
    \item{
      Langfelder, P., Luo, R., Oldham, M. C. & Horvath, S. \emph{Is my
      network module preserved and reproducible?} PLoS Comput. Biol. 
      \strong{7}, e1001057 (2011). 
    }
    \item{
      Phipson, B. & Smyth, G. K. \emph{Permutation P-values should never be 
      zero: calculating exact P-values when permutations are randomly drawn.}
      Stat. Appl. Genet. Mol. Biol. \strong{9}, Article39 (2010). 
    }
    \item{
      Langfelder, P. & Horvath, S. \emph{WGCNA: an R package for weighted 
      correlation network analysis.} BMC Bioinformatics \strong{9}, 559 
      (2008).
    }
  }
}

